{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "At this stage, first of all, wrapper functions of multiple different stemming algorithms were written for the data we have.\n",
    "\n",
    "Before a preprocess operation, separators, operators, punctuations and non-printable characters were removed. Then on the basis of being optional, a normalization can be performed with or without a stop count, or with a different stemming type option.\n",
    "\n",
    "As a matter of fact, at the last stage, we will compare the result with different parameters in this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from trstop import trstop\n",
    "import string\n",
    "from typing import List\n",
    "from jpype import JClass, JString, getDefaultJVMPath, shutdownJVM, startJVM, java\n",
    "from examples import DATA_PATH, ZEMBEREK_PATH\n",
    "from pathlib import Path\n",
    "\n",
    "startJVM(getDefaultJVMPath(), '-ea',\n",
    "         '-Djava.class.path=%s' % (ZEMBEREK_PATH))\n",
    "\n",
    "TurkishMorphology: JClass = JClass('zemberek.morphology.TurkishMorphology')\n",
    "TurkishSentenceNormalizer: JClass = JClass(\n",
    "    'zemberek.normalization.TurkishSentenceNormalizer'\n",
    ")\n",
    "\n",
    "\n",
    "Paths: JClass = JClass('java.nio.file.Paths')\n",
    "\n",
    "\n",
    "def stem(text: str) -> str:\n",
    "    morphology = TurkishMorphology.createWithDefaults()\n",
    "\n",
    "    analysis: java.util.ArrayList = (\n",
    "        morphology.analyzeAndDisambiguate(text).bestAnalysis()\n",
    "    )\n",
    "\n",
    "    pos: List[str] = []\n",
    "    for i, analysis in enumerate(analysis, start=1):\n",
    "        pos.append(\n",
    "            f'{str(analysis.getLemmas()[0])}'\n",
    "        )\n",
    "    return ' '.join(pos)\n",
    "\n",
    "\n",
    "def normalize(text: str) -> str:\n",
    "\n",
    "    normalizer = TurkishSentenceNormalizer(\n",
    "        TurkishMorphology.createWithDefaults(),\n",
    "        Paths.get(str(DATA_PATH.joinpath('normalization'))),\n",
    "        Paths.get(str(DATA_PATH.joinpath('lm', 'lm.2gram.slm'))),\n",
    "    )\n",
    "\n",
    "    return normalizer.normalize(JString(text))\n",
    "\n",
    "\n",
    "def fps(text: str, n) -> str:\n",
    "    return ' '.join([w[: n] for w in text.split()])\n",
    "\n",
    "\n",
    "def preprocess(x, stemming=None):\n",
    "    x = x.strip()\n",
    "    x = normalize(x)\n",
    "    x = remove_punctuation(x)\n",
    "    x = tokenize(x)\n",
    "    x = remove_stopwords(x)\n",
    "    if stemming == 'zemberek':\n",
    "        x = tokenize(stem(' '.join(x)))\n",
    "    elif stemming == 'fps5':\n",
    "        x = tokenize(fps(' '.join(x), 5))\n",
    "    elif stemming == 'fps7':\n",
    "        x = tokenize(fps(' '.join(x), 7))\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def remove_punctuation(x):\n",
    "    return ''.join([w for w in x if w not in string.punctuation])\n",
    "\n",
    "\n",
    "def tokenize(x):\n",
    "    return re.split(r'\\W+', x)\n",
    "\n",
    "\n",
    "def remove_stopwords(x):\n",
    "    return [w for w in x if not trstop.is_stop_word(w)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example for Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeff \\n\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t HGS İhlalli Geçiş Bilgileri Sorgulama Hizmeti ile ihlalli geçişlerin olup olmadığının sorgulanacağının belirtildiği açıklamada, \"İhlalli geçişleri var ise son 5 tanesini ve ihlalli geçiş ücretleriyle ihlal ceza tutarlarını görebilmektedir.   Bu hizmetle HGS kullanıcılarının geçiş tarihinden itibaren 15 günlük yasal süresi içinde varsa ihlallerini görerek yeterli bakiyeyi yatırması ve geçişlerin cezaya girmesinin önlenmesi amaçlanmaktadır\" ifadesi kullanıldı. \\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = 'TTC-3600/TTC-3600_Orj/ekonomi/c (1).txt'\n",
    "\n",
    "with open(file_name) as file:\n",
    "    text = file.read()\n",
    "\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'hgs',\n",
       " 'ihlalli',\n",
       " 'ihlali',\n",
       " 'geçişlerin',\n",
       " 'sorgulanacağının',\n",
       " 'belirtildiği',\n",
       " 'ihlali',\n",
       " 'geçişleri',\n",
       " '5',\n",
       " 'tanesini',\n",
       " 'ihlali',\n",
       " 'ücretleriyle',\n",
       " 'tutarlarını',\n",
       " 'görebilmektedir',\n",
       " 'hizmetle',\n",
       " 'hgs',\n",
       " 'kullanıcılarının',\n",
       " '15',\n",
       " 'ihlallerini',\n",
       " 'bakiyeyi',\n",
       " 'yatırması',\n",
       " 'geçişlerin',\n",
       " 'cezaya',\n",
       " 'girmesinin',\n",
       " 'önlenmesi',\n",
       " 'amaçlanmaktadır',\n",
       " '']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hgs',\n",
       " 'ihlal',\n",
       " 'ihlal',\n",
       " 'geç',\n",
       " 'sorgula',\n",
       " 'belir',\n",
       " 'ihlal',\n",
       " 'geç',\n",
       " '5',\n",
       " 'tane',\n",
       " 'ihlal',\n",
       " 'ücret',\n",
       " 'tutar',\n",
       " 'gör',\n",
       " 'hizmet',\n",
       " 'hgs',\n",
       " 'kullan',\n",
       " '15',\n",
       " 'ihlal',\n",
       " 'bakiye',\n",
       " 'yatır',\n",
       " 'geç',\n",
       " 'ceza',\n",
       " 'gir',\n",
       " 'önle',\n",
       " 'amaçla']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(text, stemming='zemberek')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hgs',\n",
       " 'ihlal',\n",
       " 'ihlal',\n",
       " 'geçiş',\n",
       " 'sorgu',\n",
       " 'belir',\n",
       " 'ihlal',\n",
       " 'geçiş',\n",
       " '5',\n",
       " 'tanes',\n",
       " 'ihlal',\n",
       " 'ücret',\n",
       " 'tutar',\n",
       " 'göreb',\n",
       " 'hizme',\n",
       " 'hgs',\n",
       " 'kulla',\n",
       " '15',\n",
       " 'ihlal',\n",
       " 'bakiy',\n",
       " 'yatır',\n",
       " 'geçiş',\n",
       " 'cezay',\n",
       " 'girme',\n",
       " 'önlen',\n",
       " 'amaçl']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(text, stemming='fps5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hgs',\n",
       " 'ihlalli',\n",
       " 'ihlali',\n",
       " 'geçişle',\n",
       " 'sorgula',\n",
       " 'belirti',\n",
       " 'ihlali',\n",
       " 'geçişle',\n",
       " '5',\n",
       " 'tanesin',\n",
       " 'ihlali',\n",
       " 'ücretle',\n",
       " 'tutarla',\n",
       " 'görebil',\n",
       " 'hizmetl',\n",
       " 'hgs',\n",
       " 'kullanı',\n",
       " '15',\n",
       " 'ihlalle',\n",
       " 'bakiyey',\n",
       " 'yatırma',\n",
       " 'geçişle',\n",
       " 'cezaya',\n",
       " 'girmesi',\n",
       " 'önlenme',\n",
       " 'amaçlan']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(text, stemming='fps7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_dataset(preprocess):\n",
    "    dataset = 'TTC-3600/TTC-3600_Orj'\n",
    "\n",
    "\n",
    "    X, y = [],  []\n",
    "    target_names = ['ekonomi', 'kultursanat', 'saglik', 'siyaset', 'spor', 'teknoloji']\n",
    "    for root, directories, files in os.walk(dataset):\n",
    "        for directory in directories:\n",
    "            for parent, _, files in os.walk(dataset + '/' + directory):\n",
    "                y += [directories.index(directory)] * len(files)\n",
    "                for file in files:\n",
    "                    with open(parent + '/' + file) as f:\n",
    "                        X.append(preprocess(f.read()))\n",
    "    y = np.array(y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-67ec54cee1ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m X_train, X_test, y_train, y_test = train_test_split(\n\u001b[1;32m      3\u001b[0m     X, y, test_size=0.2, random_state=41)\n\u001b[1;32m      4\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# use seaborn plotting style\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-62d86477de82>\u001b[0m in \u001b[0;36mget_dataset\u001b[0;34m(preprocess)\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                         \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-8efba9b8d2f6>\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(x, stemming)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstemming\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_punctuation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-8efba9b8d2f6>\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     normalizer = TurkishSentenceNormalizer(\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mTurkishMorphology\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateWithDefaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mPaths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoinpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'normalization'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X, y = get_dataset(preprocess=preprocess)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=41)\n",
    "sns.set()  # use seaborn plotting style\n",
    "\n",
    "model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "# Train the model using the training data\n",
    "model.fit(X_train, y_train)\n",
    "# Predict the categories of the test data\n",
    "predicted_categories = model.predict(X_test)\n",
    "\n",
    "\n",
    "# plot the confusion matrix\n",
    "mat = confusion_matrix(y_test, predicted_categories)\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt=\"d\",\n",
    "            xticklabels=target_names, yticklabels=target_names)\n",
    "plt.xlabel(\"true labels\")\n",
    "plt.ylabel(\"predicted label\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"The accuracy is {}\".format(accuracy_score(y_test, predicted_categories)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
